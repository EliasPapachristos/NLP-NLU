{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spacy\n",
    "\n",
    "If you cannot import it, then open Anaconda Prompt as an Administrator (right click on Anaconda Prompt -> More -> Open as Admin) and then:\n",
    "\n",
    "- conda install -c conda-forge spacy\n",
    "- python -m spacy download en\n",
    "\n",
    "This should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tell me, Muse, of that man of many resources, who wandered far and wide, after sacking the holy citadel of Troy. Many the men whose cities he saw, whose ways he learned. Many the sorrows he suffered at sea, while trying to bring himself and his friends back alive. Yet despite his wishes he failed to save them, because of their own un-wisdom, foolishly eating the cattle of Helios, the Sun, so the god denied them their return. Tell us of these things, beginning where you will, Goddess, Daughter of Zeus.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformating the spaCy parse of that sentence as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "rows = []\n",
    "\n",
    "for t in doc:\n",
    "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection (SBD) â€“ also known as Sentence Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Now, all the others, who had escaped destruction, had reached their homes, and were free of sea and war. \\\n",
    "        He alone, longing for wife and home, Calypso, the Nymph, kept in her echoing cavern, desiring him for a husband. \\\n",
    "        Not even when the changing seasons brought the year the gods had chosen for his return to Ithaca was he free from danger, \\\n",
    "        and among friends. \\\n",
    "        Yet all the gods pitied him, except Poseidon, \\\n",
    "        who continued his relentless anger against godlike Odysseus until he reached his own land at last.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Destructive Tokenization - Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\">\", sent.start, sent.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[25:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[45]\n",
    "print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "def get_text (url):\n",
    "    buf = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "        \n",
    "        for p in soup.find_all(\"p\"):\n",
    "            buf.append(p.get_text())\n",
    "\n",
    "        return \"\\n\".join(buf)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic = {}\n",
    "lic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))\n",
    "lic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-2.0\"))\n",
    "lic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))\n",
    "\n",
    "for sent in lic[\"bsd\"].sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\"mit\", \"asl\"],\n",
    "    [\"asl\", \"bsd\"],\n",
    "    [\"bsd\", \"mit\"]\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    print(a, b, lic[a].similarity(lic[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Now, all the others, who had escaped destruction, had reached their homes, and were free of sea and war. He alone, longing for wife and home, Calypso, the Nymph, kept in her echoing cavern, desiring him for a husband. Not even when the changing seasons brought the year the gods had chosen for his return to Ithaca was he free from danger, and among friends. Yet all the gods pitied him, except Poseidon, who continued his relentless anger against godlike Odysseus until he reached his own land at last.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Name Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have problems with Spacy_Wordnet then:\n",
    "    \n",
    "    pip install spacy-wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "\n",
    "print(\"before\", nlp.pipe_names)\n",
    "\n",
    "if \"WordnetAnnotator\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n",
    "    \n",
    "print(\"after\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfom Automatic Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"withdraw\")[0]\n",
    "token._.wordnet.synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.wordnet.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.wordnet.wordnet_domains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particular Domain or Set of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"finance\", \"banking\"]\n",
    "sentence = nlp(u\"I want to withdraw 5.000 euros.\")\n",
    "\n",
    "enriched_sent = []\n",
    "\n",
    "for token in sentence:\n",
    "    # get synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
    "    \n",
    "    if synsets:\n",
    "        lemmas_for_synset = []\n",
    "        \n",
    "        for s in synsets:\n",
    "            # get synset variants and add to the enriched sentence\n",
    "            lemmas_for_synset.extend(s.lemma_names())\n",
    "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
    "    else:\n",
    "        enriched_sent.append(token.text)\n",
    "\n",
    "print(\" \".join(enriched_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "\n",
    "if \"merge_entities\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "\n",
    "if \"merge_noun_chunks\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
    "\n",
    "convention_df = st.SampleCorpora.ConventionData2012.get_data() \n",
    "corpus = (st.CorpusFromPandas(convention_df,\n",
    "                             category_col=\"party\",\n",
    "                             text_col=\"text\",\n",
    "                             nlp=st.whitespace_nlp_with_sentences).build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category=\"democrat\",\n",
    "    category_name=\"Democratic\",\n",
    "    not_category_name=\"Republican\",\n",
    "    width_in_pixels=1000,\n",
    "    metadata=convention_df[\"speaker\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "file_name = \"foo.html\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
